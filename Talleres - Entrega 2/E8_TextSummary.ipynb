{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Text Summary\n",
        "\n",
        "Text summarization is important in the field of machine learning and natural language processing for several reasons:\n",
        "\n",
        "1. **Information Retrieval:** Text summarization helps users quickly grasp the main points or key information from a large document, making it easier to decide whether to read the full document or not. This is particularly valuable in scenarios where individuals are inundated with vast amounts of textual data, such as news articles, research papers, or social media posts.\n",
        "\n",
        "2. **Time Efficiency:** Summarization algorithms can process and generate summaries much faster than humans can read and summarize large texts. This saves time and allows users to focus their attention on the most relevant content.\n",
        "\n",
        "3. **Content Extraction:** Text summarization can automatically extract essential information from a document, enabling applications like content recommendation, keyword extraction, and topic modeling.\n",
        "\n",
        "4. **Content Generation:** Summarization models can be used to generate concise, coherent, and informative summaries for various purposes, such as creating abstracts for research papers, news article headlines, or social media post previews.\n",
        "\n",
        "5. **Multilingual Support:** Text summarization can be applied to texts in multiple languages, making it a valuable tool for global communication and information retrieval.\n",
        "\n",
        "6. **Personalization:** Summarization can be personalized to individual preferences. Machine learning models can learn from user feedback to generate summaries that align more closely with a user's interests and priorities.\n",
        "\n",
        "7. **Scalability:** As the volume of digital content continues to grow, automated summarization becomes crucial for scaling information processing and retrieval. Machine learning-based summarization models can adapt and handle large volumes of text efficiently.\n",
        "\n",
        "8. **Legal and Compliance:** In legal and regulatory contexts, automated summarization can help organizations review contracts, policies, and legal documents to ensure compliance and identify critical clauses or information.\n",
        "\n",
        "9. **Search Engine Optimization (SEO):** Summarized content can be used to create concise and engaging snippets for search engine results, improving the discoverability of web content.\n",
        "\n",
        "10. **Content Creation:** Summarization can be integrated into content creation tools, helping authors and content creators generate concise and informative content more efficiently.\n",
        "\n",
        "Overall, text summarization is an essential component of machine learning and natural language processing, enabling efficient information retrieval, content extraction, and content generation across a wide range of applications and industries. It plays a critical role in handling the ever-increasing amount of textual data available in the digital age."
      ],
      "metadata": {
        "id": "n2NseOxJAIFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Exercise:\n",
        "\n",
        "Now, as a data scientist expert in NLP, you are asked to create a model to be able to summarize text in Spanish. Your stakeholders will pass you an article and your model should summarize it."
      ],
      "metadata": {
        "id": "0voj5DTPAukU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 sumy langdetect nltk\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from langdetect import detect\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langdetect import detect\n",
        "from transformers import pipeline\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# URL del artículo\n",
        "url = \"https://time.com/collection/time100-ai/6309026/geoffrey-hinton/\"\n",
        "\n",
        "# Realizar una solicitud HTTP para obtener el contenido de la página\n",
        "response = requests.get(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hKs-JhtBwzf",
        "outputId": "9ff3b7d2-fd94-4f67-95d5-c606dd1d61cd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: sumy in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.6.2)\n",
            "Requirement already satisfied: breadability>=0.1.20 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.1.20)\n",
            "Requirement already satisfied: pycountry>=18.2.23 in /usr/local/lib/python3.10/dist-packages (from sumy) (24.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### CODIGO USANDO PIPELINE SUMMARIZER (HUGGING FACE) ######\n",
        "##### EL CÓDIGO SE ESTABLECE PARA RECIBIR TAMBIEN ARTÍCULOS EN ESPAÑOL #####\n",
        "\n",
        "# Verificar si la solicitud fue exitosa\n",
        "if response.status_code == 200:\n",
        "    # Analizar el contenido HTML de la página con BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Encontrar el contenido del artículo (inspeccionar el HTML para encontrar la estructura adecuada)\n",
        "    article_content = soup.find(\"div\", {\"class\": \"article-content\"})\n",
        "\n",
        "    # Extraer el texto del artículo\n",
        "    article_text = \"\"\n",
        "    if article_content is not None:\n",
        "        for paragraph in article_content.find_all(\"p\"):\n",
        "            article_text += paragraph.get_text() + \"\\n\"\n",
        "\n",
        "        # Detectar idioma\n",
        "        def detectar_idioma(texto):\n",
        "            return detect(texto)\n",
        "\n",
        "        # Resumir texto\n",
        "        def resumir_texto(texto):\n",
        "            idioma = detectar_idioma(texto)\n",
        "            summarizer = pipeline(\"summarization\")  # pipeline de summarization\n",
        "\n",
        "            # Dividir texto\n",
        "            fragmentos = []\n",
        "            max_length = 1024  # Longitud máxima permitida por el modelo\n",
        "            for i in range(0, len(texto), max_length):\n",
        "                fragmentos.append(texto[i:i + max_length])\n",
        "\n",
        "            resúmenes = []\n",
        "            try:\n",
        "                for fragmento in fragmentos:\n",
        "                    resumen = summarizer(fragmento, max_length=150, min_length=30, do_sample=False)\n",
        "                    if resumen:\n",
        "                        resúmenes.append(resumen[0]['summary_text'])\n",
        "\n",
        "                # Combinar resúmenes\n",
        "                resumen_final = \" \".join(resúmenes)\n",
        "                print(f\"Resumen:\\n\", resumen_final)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error al resumir el texto: {e}\")\n",
        "\n",
        "        # Función de resumen\n",
        "        resumir_texto(article_text)\n",
        "    else:\n",
        "        print(\"No se encontró el contenido del artículo. Verifica la estructura del HTML.\")\n",
        "else:\n",
        "    print(\"Error al obtener la página:\", response.status_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lob9ZQWom6yU",
        "outputId": "6344d4fd-193e-43ab-bd48-dcb0b1e84891"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Your max_length is set to 150, but your input_length is only 44. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumen:\n",
            "  Geoffrey Hinton, 76, has spent his career trying to build AI systems that model the human brain . He had always believed that the brain was better than the machines that he and others were building . But in February, he realized “the digital intelligence we’ve got now may be better than . the brain already. It’s just not scaled up quite as big”  Hinton worries about what could happen once AI systems are scaled up to the size of human brains . “This stuff will get smarter than us and take over,” he says . Hinton comes from a long line of luminaries, with relatives including the mathematician Mary Everest Boole and logician George Boole .  In the 1970s, artificial intelligence was going through a period of dampened enthusiasm now referred to as the “AI winter .” In this unfashionable field, Hinton pursued an unpopular idea: neural networks . He published pathbreaking research for which he was awarded the 2018 Turing Award .  In 2012, Hinton and two of his graduate students, Alex Krizhevsky and Ilya Sutskever, won ImageNet . They dominated the competition in which they built the most accurate image-recognition AI systems . They set up a shell company called DNN-research to auction their expertise and four tech firms bid tens of millions for the company . Hinton chose Google over the final bidder, Baidu .  Hinton does not know how to prevent superhuman AI systems from taking over . If there’s any hope, he says, it lies with the next generation, noting that he feels too old to continue contributing to research . Hinton declined Google's offer to take a policy role at the company .  Hinton's cousin Joan Hinton was one of the only women scientists to work on the Manhattan Project . After the nuclear weapons that she helped to create were dropped on Hiroshima and Nagasaki, she became a peace activist . Hinton’s own retirement p.n.talks with policymakers to raise public awareness of AI .  He intends to rediscover carpentry and take long walks . His lans are less strident but likewise bucolic: \"I love carpentry,\" he says .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### CODIGO USANDO SUMMARIZER LSA ######\n",
        "##### EL CÓDIGO SE ESTABLECE PARA RECIBIR TAMBIEN ARTÍCULOS EN ESPAÑOL #####\n",
        "\n",
        "if response.status_code == 200:\n",
        "    # Analizar el contenido HTML de la página con BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Encontrar el contenido del artículo (puedes inspeccionar el HTML de la página para encontrar la estructura adecuada)\n",
        "    article_content = soup.find(\"div\", {\"class\": \"article-content\"})\n",
        "\n",
        "    # Extraer el texto del artículo\n",
        "    article_text = \"\"\n",
        "    for paragraph in article_content.find_all(\"p\"):\n",
        "        article_text += paragraph.get_text() + \"\\n\"\n",
        "\n",
        "    # Detectar idioma\n",
        "    def detectar_idioma(texto):\n",
        "        return detect(texto)\n",
        "\n",
        "    # Resumir texto\n",
        "    def resumir_texto(texto, num_oraciones=10):\n",
        "        idioma = detectar_idioma(texto)\n",
        "        if idioma == 'es':\n",
        "            tokenizador_idioma = \"spanish\"\n",
        "        elif idioma == 'en':\n",
        "            tokenizador_idioma = \"english\"\n",
        "        else:\n",
        "            print(\"Idioma no soportado.\")\n",
        "            return\n",
        "\n",
        "        # analizador del texto\n",
        "        parser = PlaintextParser.from_string(texto, Tokenizer(tokenizador_idioma))\n",
        "\n",
        "        # Método de resumen LSA\n",
        "        summarizer = LsaSummarizer()\n",
        "\n",
        "        # Resumen del texto\n",
        "        resumen = summarizer(parser.document, num_oraciones)\n",
        "\n",
        "        # Imprimir\n",
        "        for sentence in resumen:\n",
        "            print(sentence)\n",
        "\n",
        "    # Función para resumir el texto extraído\n",
        "    resumir_texto(article_text)\n",
        "\n",
        "else:\n",
        "    print(\"Error al obtener la página:\", response.status_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQQu9XcaY7L7",
        "outputId": "8bc6fbce-de2d-4009-8c0f-bff1ba26a239"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alarmed, Hinton left his post as VP and engineering fellow in May and gave a flurry of interviews in which he explained that he had left in order to be able to speak freely on the dangers of AI—and his regrets over helping bring that technology into existence.\n",
            "Each time he replied, “Give me another six months and I’ll prove to you that it works.” Upon completion of his Ph.D., Hinton moved to the U.S., where more funding was available for his research.\n",
            "Toronto has become Hinton’s home base; he travels relatively infrequently because back problems prevent him from sitting down.\n",
            "In 2012, Hinton and two of his graduate students, Alex Krizhevsky and Ilya Sutskever, now chief scientist at OpenAI, entered ImageNet, a once annual competition in which researchers competed to build the most accurate image-recognition AI systems.\n",
            "He and his two students began receiving lucrative offers from big tech companies.\n",
            "His work has potentially hastened the future he fears, in which AI becomes superhuman with disastrous results for humans.\n",
            "In an interview with the New York Times, Hinton said, “I console myself with the normal excuse: If I hadn’t done it, somebody else would have.” Hinton does not know how to prevent superhuman AI systems from taking over.\n",
            "“I’ve never been very good at or interested in policy issues,” he tells TIME.\n",
            "While on a theoretical level he now grasps the risks from AI, Hinton says that his emotions haven’t yet caught up.\n",
            "Hinton’s own retirement plans are less strident but likewise bucolic: he intends to rediscover carpentry and take long walks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### CODIGO USANDO SEQ2SEQ (BART) #####\n",
        "##### EL CÓDIGO SE ESTABLECE PARA RECIBIR TAMBIEN ARTÍCULOS EN ESPAÑOL #####\n",
        "\n",
        "if response.status_code == 200:\n",
        "    # Analizar el contenido HTML de la página con BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Encontrar el contenido del artículo\n",
        "    article_content = soup.find(\"div\", {\"class\": \"article-content\"})\n",
        "\n",
        "    # Extraer el texto del artículo\n",
        "    article_text = \"\"\n",
        "    if article_content:\n",
        "        for paragraph in article_content.find_all(\"p\"):\n",
        "            article_text += paragraph.get_text() + \"\\n\"\n",
        "\n",
        "    # Article_text como cadena\n",
        "    article_text = article_text.strip()\n",
        "\n",
        "    # Dividir el texto\n",
        "    max_input_length = 1024\n",
        "    article_parts = [article_text[i:i + max_input_length] for i in range(0, len(article_text), max_input_length)]\n",
        "\n",
        "    # Seq2Seq BART\n",
        "    resumen_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "    # Resumen del texto\n",
        "    def resumir_texto(texto):\n",
        "        resúmenes = []\n",
        "        for part in texto:\n",
        "            idioma = detect(part)  # Detectar idioma\n",
        "            if idioma in ['es', 'en']:  # Español e inglés\n",
        "                # Ajuste max_length y min_length\n",
        "                summary = resumen_pipeline(part, max_length=80, min_length=20, do_sample=False)\n",
        "                resúmenes.append(summary[0]['summary_text'])\n",
        "\n",
        "        resumen_completo = \"\\n\".join(resúmenes)\n",
        "        print(\"Resumen:\\n\", resumen_completo)\n",
        "\n",
        "    if article_parts:\n",
        "        resumir_texto(article_parts)\n",
        "    else:\n",
        "        print(\"No se encontró contenido para resumir.\")\n",
        "else:\n",
        "    print(\"Error al obtener la página:\", response.status_code)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU7KXTpAbTOK",
        "outputId": "ce55e8d2-c617-4bbc-b525-14c32e4997f6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 80, but your input_length is only 43. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumen:\n",
            " Geoffrey Hinton is one of the most influential AI researchers of the past 50 years. Hinton, 76, has spent his career trying to build AI systems that model the human brain. Given the current rate at which AI companies are increasing the size of models, it could be less than five years until AI systems have 100 trillion connections.\n",
            "Hinton worries about what could happen once AI systems are scaled up to the size of human brains. He worries about the prospect of humanity being wiped out by the technology he helped create. “This stuff will get smarter than us and take over,” he says.\n",
            "In the 1970s, artificial intelligence, after failing to live up to its postwar promise, was going through a period of dampened enthusiasm. Hinton pursued an unpopular idea: neural networks, which mimicked the structure of the human brain. He published pathbreaking research, for which he was awarded the 2018 Turing Award.\n",
            "In 2012, Hinton and two of his graduate students, Alex Krizhevsky and Ilya Sutskever, now chief scientist at OpenAI, dominated the ImageNet competition. They set up a shell company called DNN-research to auction their expertise, and four tech firms bid tens of millions for the company. Hinton chose Google over the final bidder, B\n",
            "Hinton does not know how to prevent superhuman AI systems from taking over. Many scientists switch to policy work later in their careers. Hinton declined Google’s offer to take such a role at the company.\n",
            "Hinton has spoken with policymakers, including officials in the U.K. Prime Minister’s office, Canadian Prime Minister Justin Trudeau, Executive Vice-President of the European Commission Margrethe Vestager, and U.S. Senators Bernie Sanders and Jon Ossoff. His cousin Joan Hinton was one of the only women scientists to work on the Manhattan Project.\n",
            "Lans says he intends to rediscover carpentry and take long walks. lans are less strident but likewise bucolic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### CODIGO USANDO SEQ2SEQ (BART) CON TRADUCCIÓN AL ESPAÑOL (MÉTODO HELSINKI) #####\n",
        "##### EL CÓDIGO SE ESTABLECE PARA RECIBIR TAMBIÉN ARTÍCULOS EN ESPAÑOL #####\n",
        "\n",
        "# Verificar si la solicitud fue exitosa\n",
        "if response.status_code == 200:\n",
        "    # Analizar el contenido HTML de la página con BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Encontrar el contenido del artículo\n",
        "    article_content = soup.find(\"div\", {\"class\": \"article-content\"})\n",
        "\n",
        "    # Extraer el texto del artículo\n",
        "    article_text = \"\"\n",
        "    if article_content:\n",
        "        for paragraph in article_content.find_all(\"p\"):\n",
        "            article_text += paragraph.get_text() + \"\\n\"\n",
        "\n",
        "    # Dividir el texto\n",
        "    max_input_length = 1024\n",
        "    article_parts = [article_text[i:i + max_input_length] for i in range(0, len(article_text), max_input_length)]\n",
        "\n",
        "    # Seq2Seq BART\n",
        "    resumen_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "    # Helsinki para traducción\n",
        "    traduccion_pipeline = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
        "\n",
        "    # Detectar idioma\n",
        "    def detectar_idioma(texto):\n",
        "        return detect(texto)\n",
        "\n",
        "    # Resumen y traducción\n",
        "    def resumir_y_traducir(texto):\n",
        "        resúmenes = []\n",
        "        for part in texto:\n",
        "            # Detectar idioma\n",
        "            idioma = detectar_idioma(part)\n",
        "            # Resumen el texto\n",
        "            resumen_en_idioma_original = resumen_pipeline(part, max_length=80, min_length=20, do_sample=False)[0]['summary_text']\n",
        "\n",
        "            if idioma == 'en':\n",
        "                resumen_traducido = traduccion_pipeline(resumen_en_idioma_original, src_lang='en', tgt_lang='es')[0]['translation_text']\n",
        "                resúmenes.append(resumen_traducido)\n",
        "            else:\n",
        "                resúmenes.append(resumen_en_idioma_original)\n",
        "\n",
        "        return \"\\n\".join(resúmenes)\n",
        "\n",
        "    # Función para Resumen y traducción\n",
        "    resumen_completo = resumir_y_traducir(article_parts)\n",
        "    print(\"Resumen:\\n\", resumen_completo)\n",
        "else:\n",
        "    print(\"Error al obtener la página:\", response.status_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg_dj63pi4om",
        "outputId": "5814e8dd-d91f-4db8-9ad2-a8b37bc1c4fd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 80, but your input_length is only 44. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumen:\n",
            " Geoffrey Hinton es uno de los investigadores de IA más influyentes de los últimos 50 años. Hinton, de 76 años, ha pasado su carrera tratando de construir sistemas de IA que modelen el cerebro humano. Dado el ritmo actual al que las compañías de IA están aumentando el tamaño de los modelos, podría ser menos de cinco años hasta que los sistemas de IA tengan 100 billones de conexiones.\n",
            "Hinton se preocupa por lo que podría pasar una vez que los sistemas de IA se amplíen al tamaño de los cerebros humanos. Se preocupa por la perspectiva de que la humanidad sea aniquilada por la tecnología que ayudó a crear. “Esto se volverá más inteligente que nosotros y se hará cargo”, dice.\n",
            "En la década de 1970, la inteligencia artificial, después de no estar a la altura de su promesa de posguerra, estaba pasando por un período de entusiasmo amortiguado. Hinton persiguió una idea impopular: redes neuronales, que imitaban la estructura del cerebro humano. Publicó investigaciones pioneras, por las que fue galardonado con el Premio Turing 2018.\n",
            "En 2012, Hinton y dos de sus estudiantes de posgrado, Alex Krizhevsky e Ilya Sutskever, ahora científico jefe de OpenAI, dominaron la competencia de ImageNet. Crearon una empresa fantasma llamada DNN-investigación para subastar su experiencia, y cuatro empresas de tecnología ofrecieron decenas de millones para la empresa. Hinton eligió Google sobre el postor final, B\n",
            "Hinton no sabe cómo evitar que los sistemas de IA sobrehumanos se hagan cargo. Muchos científicos cambian a la política de trabajo más tarde en sus carreras. Hinton rechazó la oferta de Google para tomar tal papel en la empresa.\n",
            "Hinton ha hablado con los responsables políticos, incluyendo funcionarios en la oficina del primer ministro del Reino Unido, el primer ministro canadiense Justin Trudeau, vicepresidente ejecutivo de la Comisión Europea Margrethe Vestager, y los senadores estadounidenses Bernie Sanders y Jon Ossoff. Su primo Joan Hinton fue una de las únicas científicas que trabajó en el Proyecto Manhattan.\n",
            "Lans dice que tiene la intención de redescubrir la carpintería y dar largos paseos. Lans es menos estridente pero también bucólico.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "StIrHeBAB07H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}